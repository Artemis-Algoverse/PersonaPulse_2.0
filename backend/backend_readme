# PersonaPulse Backend - Complete Implementation

## System Architecture
[User Social Media Profiles] 
       ↓ (Scraping using OSINT/SOCMINT)
[Raw Data Database - UserProfile] 
       ↓ (Prompt fed to GenAI model)
[Processed Personality DB: Interests + OCEAN]
       ↓
[Event Recommendation System]

## Files Created:

### Core Application
- `app.py` - Flask API server with all endpoints
- `cli.py` - Command line interface for interactive usage
- `demo.py` - System demonstration and health check
- `config.py` - Configuration management
- `models.py` - Database models (UserProfile, PersonalityData, ScrapingLog)
- `services.py` - Business logic and service layer

### Data Processing
- `ai_analyzer.py` - Gemini AI personality analysis
- `scheduler.py` - APScheduler for automated updates

### Social Media Scrapers
- `scrapers/instagram_scraper.py` - Instagram data extraction
- `scrapers/twitter_scraper.py` - Twitter API integration
- `scrapers/reddit_scraper.py` - Reddit PRAW integration  
- `scrapers/linkedin_scraper.py` - LinkedIn Selenium scraper
- `scrapers/__init__.py` - Scrapers package initialization

### Setup & Configuration
- `requirements.txt` - Python dependencies
- `.env.example` - Environment variables template
- `README.md` - Complete documentation
- `setup.bat` / `setup.sh` - Automated setup scripts

## Quick Start:
1. Run `python demo.py` to check system status
2. Run `python cli.py` for interactive usage
3. Run `python app.py` for API server

## Features Implemented:
✅ Multi-platform social media scraping (Instagram, Twitter, Reddit, LinkedIn)
✅ AI-powered personality analysis using Gemini 2.0 Flash  
✅ OCEAN personality trait scoring
✅ Interest keyword extraction
✅ Automated daily data updates
✅ RESTful API with comprehensive endpoints
✅ Command line interface
✅ Comprehensive logging and error handling
✅ Database models with relationships
✅ Environment-based configuration


# PersonaPulse Backend

This is the backend system for PersonaPulse - a platform that analyzes social media profiles to understand personality traits and interests.

## Features

- **Social Media Scraping (OSINT/SOCMINT)**:
  - Instagram: Bio, posts, hashtags, follower count
  - Twitter: Tweets, follower count, profile data
  - Reddit: Posts, comments, karma
  - LinkedIn: About section, connections, recent activity

- **AI-Powered Personality Analysis**:
  - Uses Google Gemini 2.0 Flash for personality analysis
  - OCEAN personality trait scoring (0-100 scale)
  - Interest keyword extraction
  - Confidence scoring for analysis accuracy

- **Automated Data Updates**:
  - Scheduled daily updates using APScheduler
  - Background processing of user data
  - Comprehensive logging system

- **RESTful API**:
  - Create and manage user profiles
  - Manual data scraping and analysis triggers
  - Comprehensive user data retrieval

## Architecture

```
PersonaPulse Backend Architecture:

[Social Media Platforms] 
       ↓ (OSINT/SOCMINT Scraping)
[Raw Data Database - UserProfile] 
       ↓ (AI Processing with Gemini)
[Processed Personality DB - PersonalityData]
       ↓
[Event Recommendation System] (Future)
```

## Setup Instructions

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Environment Configuration

Copy `.env.example` to `.env` and fill in your API keys:

```bash
cp .env.example .env
```

Required API keys:
- **Gemini API Key**: Get from Google AI Studio
- **Twitter API**: Get from Twitter Developer Portal
- **Reddit API**: Create app at Reddit App Preferences
- **Instagram/LinkedIn**: Use your login credentials

### 3. Database Setup

The system uses SQLite by default (no setup required). Configure your database URL in `.env`:

```
DATABASE_URL=sqlite:///personapulse.db
```

For production, you can use PostgreSQL or other databases:
```
DATABASE_URL=postgresql://username:password@localhost:5432/personapulse_db
```
DATABASE_URL=sqlite:///personapulse.db
```

### 4. Run the Application

Start the Flask API server:
```bash
python app.py
```

The API will be available at `http://localhost:5000`

## Usage

### Web Interface

The backend is designed to work with the React frontend. Start both:

1. **Backend API:**
   ```bash
   cd backend
   python app.py
   ```

2. **Frontend (in another terminal):**
   ```bash
   cd frontend/PersonaPulse2.0
   npm run dev
   ```

3. **Access the application:**
   Open `http://localhost:5173` in your browser

### API Endpoints

- `POST /api/users` - Create new user profile and analyze personality
- `GET /api/users` - Get all users
- `GET /api/users/<id>` - Get specific user profile
- `POST /api/users/<id>/scrape` - Manual data scraping
- `POST /api/users/<id>/analyze` - Manual personality analysis
- `GET /api/scheduler/status` - Get scheduler status
- `POST /api/scheduler/start` - Start scheduled updates

### Example API Usage

Create a new user through the web interface or via API:

```bash
# Create a new user
curl -X POST http://localhost:5000/api/users \
  -H "Content-Type: application/json" \
  -d '{
    "instagram": "username",
    "twitter": "username",
    "reddit": "username",
    "linkedin": "https://linkedin.com/in/username"
  }'

# Get all users
curl http://localhost:5000/api/users
```

The web interface provides a user-friendly form to input social media IDs and view results.

## Database Schema

### UserProfile Table
- `unique_persona_pulse_id` - Unique identifier
- `insta_id`, `linkedin_id`, `reddit_id`, `twitter_id` - Social media IDs
- `insta_bio`, `linkedin_about` - Profile information
- `insta_posts_hashtags`, `reddit_posts`, `twitter_posts` - Content data
- Metadata: follower counts, karma, connections

### PersonalityData Table
- `unique_persona_pulse_id` - Links to UserProfile
- `list_of_interest_keywords` - JSON array of interests
- OCEAN scores: `openness`, `conscientiousness`, `extraversion`, `agreeableness`, `neuroticism`
- `confidence_score` - Analysis confidence (0-100)
- `dominant_traits` - Top personality traits

### ScrapingLog Table
- Logs all scraping activities
- Tracks success/failure status
- Error messages and item counts

## Technologies Used

- **Python 3.8+**
- **Flask** - Web framework
- **SQLAlchemy** - Database ORM
- **SQLite** - Primary database (file-based, no setup required)
- **Instaloader** - Instagram scraping
- **Web Scraping** - Twitter and Reddit data collection
- **PRAW** - Reddit API
- **Selenium** - LinkedIn scraping
- **Google Generative AI** - Personality analysis
- **APScheduler** - Task scheduling

## Security Considerations

- Store API keys securely in environment variables
- Use rate limiting for social media APIs
- Implement proper error handling and logging
- Consider using headless browsers for scraping
- Respect platform terms of service and rate limits

## Future Enhancements

- Event recommendation system based on personality data
- Real-time personality updates
- Multi-language support
- Advanced privacy controls
- Machine learning model training
- Sentiment analysis integration

## Contributing

1. Fork the repository
2. Create a feature branch
3. Implement your changes
4. Add tests for new functionality
5. Submit a pull request

## License

This project is for educational and research purposes. Ensure compliance with social media platform terms of service when using this system.


# PersonaPulse Backend - Updated Implementation

## System Architecture (Updated)
```
[Web Form Input: Multiple Users' Social Media IDs] 
       ↓ (Stored in SQLite Database)
[UserProfile Database: Raw Social Media Data] 
       ↓ (Public Data Scraping using Web Scraping)
[Scraped Social Media Content]
       ↓ (AI Analysis using Gemini)
[PersonalityData Database: OCEAN + Interests]
       ↓
[Event Recommendation System] (Future)
```

## Key Changes Made

### ✅ **Removed Personal Credentials Requirement**
- No need for Instagram username/password
- No need for LinkedIn email/password
- Only API keys required for Twitter and Reddit
- Instagram scraping uses public data only
- LinkedIn scraping uses public profile information

### ✅ **Multi-User Support via Web Form**
- Users input their social media usernames through the React frontend
- Each user gets a unique persona pulse ID
- All data stored in SQLite database
- No command-line input needed

### ✅ **Updated Environment Configuration**
Only these API keys are needed in `.env`:
```bash
# Required API Keys
GEMINI_API_KEY=your_gemini_api_key_here
TWITTER_BEARER_TOKEN=your_twitter_bearer_token
TWITTER_CONSUMER_KEY=your_twitter_consumer_key
TWITTER_CONSUMER_SECRET=your_twitter_consumer_secret
TWITTER_ACCESS_TOKEN=your_twitter_access_token
TWITTER_ACCESS_TOKEN_SECRET=your_twitter_access_token_secret
REDDIT_CLIENT_ID=your_reddit_client_id
REDDIT_CLIENT_SECRET=your_reddit_client_secret

# Database
DATABASE_URL=sqlite:///personapulse.db
```

### ✅ **Updated Scraping Approach**

#### Instagram Scraper
- Uses `instaloader` for public profile data
- No login required
- Scrapes: bio, follower count, public posts, hashtags
- Rate limited for respectful access

#### Twitter Scraper  
- Uses Twitter API v2 with bearer token
- Scrapes: tweets, follower count, profile data
- Full API access with proper authentication

#### Reddit Scraper
- Uses PRAW (Reddit API)
- Scrapes: posts, comments, karma
- API-based access with client credentials

#### LinkedIn Scraper
- Uses requests + BeautifulSoup for public data
- No selenium/browser automation needed
- Scrapes: public profile information, headlines
- Limited data due to LinkedIn's restrictions

### ✅ **Data Flow**
1. **User Input**: Frontend form collects social media usernames
2. **Database Storage**: UserProfile created with unique ID
3. **Scraping**: Each platform scraped using user-provided usernames
4. **AI Analysis**: Gemini analyzes combined social media data
5. **Results Storage**: PersonalityData saved with OCEAN scores and interests
6. **Display**: Results shown in beautiful web interface

## Files Updated:

### Configuration Files
- `.env.example` - Removed personal credential requirements
- `config.py` - Removed Instagram/LinkedIn credential configs
- `requirements.txt` - Removed selenium and webdriver-manager

### Scraper Files
- `scrapers/instagram_scraper.py` - Public data access only
- `scrapers/linkedin_scraper.py` - BeautifulSoup-based public scraping
- `services.py` - Removed credential dependencies
- `scheduler.py` - Updated initialization

### Benefits of New Approach

✅ **Privacy-Friendly**: No personal credentials stored
✅ **Scalable**: Can handle multiple users easily
✅ **Maintainable**: Simpler setup, fewer dependencies
✅ **Compliant**: Uses public data and official APIs
✅ **User-Friendly**: All input through web interface
✅ **Reliable**: Less prone to authentication issues

### Limitations

⚠️ **Instagram**: Limited to public profiles and posts
⚠️ **LinkedIn**: Very limited public data available
⚠️ **Rate Limits**: Slower scraping to respect platform limits
⚠️ **Public Data Only**: Cannot access private/protected content

### Setup Instructions

1. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Configure API Keys**:
   ```bash
   cp .env.example .env
   # Edit .env with your API keys (no personal credentials needed)
   ```

3. **Run Application**:
   ```bash
   python app.py
   ```

4. **Access Frontend**:
   ```bash
   cd frontend/PersonaPulse2.0
   npm install && npm run dev
   ```

The system now works entirely through the web interface, collecting usernames from users and processing their public social media data for personality analysis.
